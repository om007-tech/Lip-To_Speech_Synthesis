# Lip-To_Speech_Synthesis
In this paper, we propose a novel lip-to-speech generative adversarial network, Visual Context Attention GAN (VCA-GAN), which can jointly model local and global lip movements during speech synthesis. 
